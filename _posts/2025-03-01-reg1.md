---
layout: single
title: "Model Selection in Linear Regression"
excerpt: ""
categories:
  - Intro-Data-Science
tags:
  - regression
toc: true
toc_label: "Table of Contents"
#  toc_icon: 
# header:
#   image:
#   teaser:
---

# Model Selection

## Error Metrics

The sum of squared error $SSE$ is the error function used for training the model, but it can't compare performance on datasets of different sizes and
is in the units of $y^2$. **Root Mean Squared Error (RMSE)**, the square root of the sum of squared residuas, is on the scale of $y$ and can compare results of
inputs of different sizes. Regardless of $n$, lower $RMSE$ means less error. 

$$RMSE = \sqrt{MSE}=\sqrt{SSE/n} =  \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$$

```python
def root_mean_squared_error(y_truth, y_pred):
    return np.sqrt(np.mean((y_truth - y_pred) ** 2))
```

Unlike before the $MSE$ does not account for the degrees of freedom $n-2$. 

**Mean Absolute Error (MAE)**, the mean absolute value of residuals, is also on the scale of the data and provides an alternative metric to compare models that is not 
the metric that the parameters were trained on.

```python
def mean_absolute_error(y_truth, y_pred):
    return np.mean(np.abs(y_truth - y_pred))
```